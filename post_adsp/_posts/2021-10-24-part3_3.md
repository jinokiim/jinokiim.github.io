---
layout: post
title: Part 3
subtitle: 
gh-repo: jinokiim/jinokiim.github.io
gh-badge: [star, fork, follow]
thumbnail-img: /assets/img/sp/dspthumb.png
cover-img: /assets/img/sp/dspbg.jpeg
tags: [ADsP]
comments: true
---  

# 데이터 분석 기획



### 데이터 마이닝 개요

#### 데이터 마이닝

기업이 보유하고 있는 일일거래 데이터, 고객 데이터, 상붐데이터 혹은 각종 마케팅 활동에 있어서의 고객반응 데이터 등과 이외의 외부 데이터를 포함하는 **모든 사용가능한 원천 데이터를 기반으로 감춰진 지식, 기대하지 못했던 경향 또는 새로운 규칙 등을 발견하고 이를 실제 비즈니스 의사결정 등에 유용한 정보로 활용**하는 일련의 작업

#### 데이터 마이닝 5단계

* 목적 정의
  * 데이터 마이닝 도입 목적을 명확하게 함
* 데이터 준비
  * **데이터 정제(Cleaning)** 를 통해 데이터의 품질 확보까지 포함
  * 필요시 **데이터 양 충분하게 확보**
* 데이터 가공
  * **목적 변수를 정의하고, 필요한 데이터를 데이터 마이닝 소프트웨어에 적용할 수 있게 가공 및 준비하는 단계**
  * 충분한 CPU와 메모리, 디스크 공간 등 개발환경 구축이 선행
* 데이터 마이닝 기법 적용
  * 모델을 목적에 맞게 선택하고 소프트웨어를 사용하는 데 필요한 값 지정
* 검증
  * 결과에 대한 검증 시행



### 대표적 데이터 마이닝 기법

* 분류(Classification)
  * 새롭게 나타난 현상을 검토하여 **기존의 분류, 정의된 집합에 배정**하는 것
  * 의사결정나무, memory-based reasoning 등
* 추정(Estimation)
  * 주어진 입력 데이터를 사용하여 **알려지지 않은 결과의 값을 추정**하는 것
  * 연속된 변수의 값을 추정, 신경망 모형
* 연관분석 (Association Analysis)
  * **'같이 풀리는 물건'** 같이 아이템의 연관성을 파악하는 분석
  * **카탈로그 배열 및 교차판매, 공격적 판촉행사 등의 마케팅 계획**
* 예측(Prediction)
  * **미래**에 대한 것을 예측, 추정하는 것을 제외하면 분류나 추정과 동일한 의미
  * 장바구니 분석, 의사결정나무, 신경망 모형
* 군집(Clustering)
  * 미리 정의된 기준이나 예시에 의해서가 아닌 **레코드 자체**가 가진 다른 레코드와의 유사성에 의해 그룹화되고 이질성에 의해 세분화 됨
  * 데이터 마이닝이나 모델링의 준비단계로서 사용됨
* 기술(Description)
  * **데이터가 가진 특징 밑 의미를 단순하게 설명하는 것**
  * 데이터가 암시하는 바에 대해 설명 및 그에 대한 답을 찾아 낼 수 있어야 함



### 데이터 분석 순서

1. 분석용 데이터 준비

2. 탐색적 분석 데이터 전처리
   * 속성간 상관관계 파악
   * 데이터 특성 파악
   * 분포 파악
   * 데이터 확인
   * 데이터 형식 변경
   * 결측값, 이상값 처리
   * 특성조작
   * 데이터 자원 축소
3. 모델링
   * 회귀분석
   * 분류분석
   * 군집분석
   * 연관분석
4. 모델 평가 및 검증
   * 결정계수(R^2)
   * F통계량, t값
   * ROC Curve
   * 오분류표
   * 실루엣, DI
5. 모델 적응 운영방안 수립



### Machine Learning Algorithms

|             |                         Unsupervised                         |                          Supervised                          |
| :---------: | :----------------------------------------------------------: | :----------------------------------------------------------: |
| Continuous  | Clustering & Dimensionality Reduction<br />SVD<br />SVD<br />PCA<br />K-means |                Regression(Linear, Polynomial)                |
| Categorical |        Association Analysis<br />(Apriori, FP-Growth)        | Classification<br />(kNN ,DecisionTrees,<br />Logistic Regression, Naive-Bayes, SVM) |

### 분류분석의 종류

1. 로지스틱 회귀
2. 의사결정나무
3. 앙상블
4. 신경망 모형
5. kNN, 베이즈분류 모형, SVM(서포트벡터기계), 유전자 알고리즘



### 로지스틱 회귀분석

* 독립변수는 연속형, **종속변수가 범주형인 경우 적용되는 회귀분석** 모형
* 종속변수가 성공/실패, 사망/생존 과 같이 **이항변수(0,1)**로 되어있을때 종속변수와 독립변수 간의 관계식을 잉요하여 두 집단 또는 그 이상의 집단을 **분류하고자 할 때 사용**되는 분석기법
  * 선형회귀
    * x값의 변화에 따른 y값의 변화를 알아냄
    * x가 1증가할 때, y는 회귀 계수 만큼 증가함
  * 로지스틱회귀
    * x값에 따른 y값의 변화량의 문제가 아님
    * 회귀계수를 해석할 때 문제가 생김

|                |     일반 선형 회귀분석      | 로지스틱 회귀분석               |
| :------------: | :-------------------------: | ------------------------------- |
|    종속변수    |         연속형 변수         | 이산형(범주형) 변수             |
| 모형 탐색 방법 | 최소자승법(LSM, 최소제곱법) | 최대우도법(MLE), 가중최소자승법 |
|   모형 검정    |       F-test, T-test        | X^2 test                        |



종속변수를 전체 실수범위로 확정하여 분석하고, sigmoid함수를 사용해 연속형 0~1로 변경

* 확률 probability
  * 0~1사이의 값
* odd(승산)
  * 성공률 / 실패율, Pi / (1-Pi), Pi = 성공률
    * 성공이 일어날 가능성이 높은 경우는 1.0보다 큰 값
    * 실패가 발생할 가능성이 높은 경우는 1.0보다 작은 값
  * **로지스틱의 회귀계수, 확률에 대해 0~∞ 로 변환한 값**
* log odds(=logit)
  * log(odds), odds 값에 log를 취함
  * 선형화의 하나, 값의 검위를 **전체 실수 범위(-∞ ~ +∞)로 확장**
  * log(odds(p)) = wx + b 형태로 선형분석이 가능해짐
* sigmoid
  * log odds 값을 연속형 0~1사이의 값으로 바꾸는 함수
  * Logistic 함수라 불리기도 하며, 비선형적 값을 얻기 위해 사용

#### 회귀 식에 대한 해석 방법이 선형회귀와 다름

* 승산비(odds ratid)
  * 관심있는 사건이 발생할 상대 비율, x=1일떄, y=1이되는 상대적 비율



### 의사 결정 나무(Decision Tree)모형

* 의사결정 규칙을 나무구조로 나타내 전체 자료를 몇 개의 소집단으로 **분류**하거나 **예측을 수행**하는 분석 방법
* 분석과정이 직관적이고 이해하기 쉬움

<용어정리>

|                 독립변수                  |                종속변수                 |
| :---------------------------------------: | :-------------------------------------: |
| * 설명변수<br />* 예측변수<br />* Feature | * 목표변수<br />* 반응변수<br />* Label |



* 특징
  * 목적은 새로운 **데이터를 분류**하거나 **값을 예측**하는 것이다.
  * 분리 변수 P차원 공간에 대한 현재 분할은 **이전 분할에 영향을 받는다**
  * 부모마디보다 자식마디의 **순수도가 증가하도록 분류나무를 형성**해 나간다(**불순도 감소**)
* 종류
  * 목표변수(=종속변수)가 이산형인 경우 분류나무(classification tree)
  * 목표변수가 연속형인 경우 회귀나무(regression tree)
* 장점
  * 구조가 단순하여 해석이 용이함
  * **비모수적 모형으로** 선형성, 정규성, 등분산성 등의 수학적 가정이 불필요함
  * **범주형(이산형)과 수치형(연속형) 변수를 모두 사용**할 수 있음
* 단점
  * 분류 기준값의 경계선 부근의 자료 값에 대해서는 오차가 큼(비연속성)
  * 로지스틱회귀와 같이 각 예측변수의 효과를 파악하기 어려움
  * 새로운 자료에 대한 예측이 불안정할 수 있음



### 의사결정나무의 결정 규칙

* 분리기준 split criterion
  * 새로운 가지를 만드는 기준?
    * 순수도가 높아지는 방향으로 분리
    * 불확실성이 낮아지는 방향
* 정지규칙 stopping rule
  * 더 이상 분리가 일어나지 않고 현재 마디가 최종마디가 되도록 하는 규칙
  * '불순도 감소량'이 아주 작을 때 정지함
* 가지치기 규칙 pruning rule
  * 어느 가지를 쳐내야 예측력이 좋은 나무가 될까?
  * **최종 노드가 너무 많으면 overfitting 가능성이 커짐, 이를 해결하기 위해 사용**
  * 가지치기 규칙은 별도 규칙을 제공하거나 경험에 의해 실행할 수 있음
  * 가지치기의 비용함수(cost function)을 **최소로 하는 분기를 찾아내도록** 학습
  * Information Gain이란 어떤 속성을 선택함으로 인해 데이터를 더 잘 구분하게 되는것을 의미함(불확실성 감소)

### 불순도 측정 지표

#### 목표변수가 범주 형일 때 사용하는 지표(분류에서 사용)

* 지니지수
  * 불순도 측정 지표, **값이 작을수록 순수도가 높음(분류가 잘 됨)**
  * 가장 작은 값을 갖는 예측 변수와 이때의 최적 분리에 의해 자식 마디 형성
  * Gini(T) = 1 - ((각 범주별수 / 전체수)의 총합)^2
* 엔트로피 지수 Entropy measure
  * 불순도 측정 지표, **가장 작은 값을 갖는 방법 선택**
  * Entropy(T) = -|_sum_{(i=1);(k);(Pilog2Pi)}
* 카이제곱 통계량의 유의 확률(p-value)



### 의사결정나무를 위한 알고리즘

의사결정나무를 위한 알고리즘은 CHAID, CART, ID2, C5.0, C4.5가 있으며 **하향식 접근 방법**을 이용한다.

|                         알고리즘                         | 이산형 목표변수(분류나무) | 연속형 목표변수(회귀나무) |
| :------------------------------------------------------: | :-----------------------: | :-----------------------: |
|      CART<br />(classification and regression tree)      |         지니지수          |        분산감소량         |
|                           C5.0                           |       엔트로피지수        |                           |
| CHAID<br />(chi-squared automatic interaction detection) | 카이제곱 통계량의 p-value | ANOVA F-통계량 - p-value  |



### 앙상블(Ensemble) 모형

* **여러 개의 분류 모형에 의한 결과를 종합하여 분류의 정확도를 높이는 방법**
* 적절한 표추출법으로 데이터에서 여러 개의 훈련용 데이터 집합을 만들어 각 데이터 집합에 하나의 분류기를 만들어 결합하는 방법
* 약하게 학습된 여러 모델들을 결합하여 사용
* **성능을 분산시키기 때문에 과적합(overfitting) 감소 효과가 있음



#### 앙상블 모형의 종류

* Voting
  * **서로 다른 여러 개 알고리즘 분류기 사용**
  * 각 모델의 결과를 취합하여 많은 결과 또는 높은 확률로 나온 것을 최종 결과로 채택하는 것
  * Hard voting
    * 각 모델의 예측 결과중 많은 것을 선택
    * 1 예측 3표, 2예측 1표 -> 1예측 선택
  * Soft voting
    * 각 모델의 클래스 확률을 구하고 평균 높은 확률을 선택
    * 1예측 : (0.9 + 0.8 + 0.3 + 0.4) / 4 = 0.6 -> 1예측 선택
    * 2예측 : (0.1 + 0.2 + 0.7 + 0.6) / 4 = 0.4
* Bagging(Bootstrap AGGregatING)
  * **서로 다른 훈련 데이터 샘플로 훈련, 서로 같은 알고리즘 분류기 결합**
  * 원 데이터에서 중복을 허용하는 크기가 같은 표본을 여러번 단순 임의 복원 추출하여 각 표본에 대해 분류기 (classifiers)를 생성하는 기법
  * **여러 모델이 병렬로 학습, 그 결과를 집계하는 방식**
  * 같은 데이터가 여러번 추출될 수도 있고, 어떤 데이터는 추출되지 않을 수 있음
  * 대표적 알고리즘 : MetaCost Algorith,
* Boostinig
  * 여러 모델이 **순차적으로 학습**
  * 이전 모델의 결과에 따라 다음 모델 표본 추출에서 분류가 잘못된 데이터에 가중치(weight)를 부여하여 표본을 추출함
  * 맞추기 어려운 문제를 맞추는데 초점이 맞춰져 있고, 이상치(Outlier)에 약함
  * 대표적 알고리즘 : AdaBoost, GradientBoost(XGBoost, Light GBM) 등
* Random Forest
  * 배깅(Bagging)에 랜덤 과정을 추가한 방법
  * 노드 내 데이터를 자식 노드로 나누는 기준을 정할 때 모든 예측변수에서 최저그이 분할을 선택하는 대신, **설명변수의 일부분만을 고려**함으로 성능을 높이는 방법 사용
  * **여러 개 의사결정 나무를 사용해**, 하나의 나무를 사용할 때보다 **과적합 문제를 피할 수 있음**

 

### k-NN(k-Nearest Neighbors)

* 새로운 데이터에 대해 주어진 **이웃의 개수(k)** 만큼 가까운 멤버들과 비교하여 결과를 판단하는 방법
* k 값에 따라 소속되는 그룹이 달라질 수 있음(k값은 hyper parameter)
* 거리를 측정해 이웃들을 뽑기 때문에 **스케일링이 중요함**
* 반응 변수가 범주형이면 불뉴, 연속형이면 회귀의 목적으로 사용됨
* 모형을 미리 만들지 않고, 새로운 데이터가 들어오면 그때부터 계산을 시작하는 lazy learning(게으른 학습)이 사용되는 지도학습 알고리즘



### SVM(Support Vector Machine)

* 서로 다른 분류에 속한 데이터 간의 간격(margin)이 최대가 되는 선을 찾아 이를 기준으로 데이터를 분류하는 모델



### 인공 신경망(ANN) 모형

* 인공신경망을 이용하면 분류 및 군집을 할 수 있음
* 인공신경망은 입력층, 은닉층, 출력층 3개의 층으로 구성되어 있음
* 각 층에 뉴런(노드)이 여러 개 포함되어 있음
* 학습 : 입력에 대한 올바른 출력이 나오도록 가중치(weights)를 조절하는 것
* 가중치 초기화는 -1.0 ~ 1.0 사이의 임의 값으로 설정하며, 가중치를 지나치게 큰 값으로 초기화하면 활성화 함수를 편향 시키게 되며, **호라성화 함수가 과적합 되는 상태를 포화상태**라고 함



### 경사하강법(Gradient descent)

* **함수 기울기를 낮은 쪽으로 계속 이동**시켜 극값에 이를 때까지 반복시키는 것
* 제시된 함수의 기울기의 **최소값을 찾아내는** 머신러닝 알고리즘
* 비용함수(cost function)을 최소화 하기 위해 parameter를 반복적으로 조정하는 과정

#### 경사하강법 과정

1. 임이의 parameter값으로 시작
2. Cost Function 계산, cost function - 모델을 구성하는 가중치 w의 함수, 시작점에서 곡선의 기울기 계산
3. parameter 값 갱신 : W = W - learning rate * 기울기미분값
4. n 번의 iteration, 최소값을 향해 수렴함, learning rate가 적절해야함



### 인공 신경망 모형의 장/단점

* 장점
  * 변수의 수가 많거나 입,출력변수 간에 **복잡한 비선형 관계에 유용**
  * **이상치 잡음에 대해서도 민감하게 반응하지 않음**
  * 입력변수와 결과변수가 연속형이나 이산형인 경우 모두 처리 가능
* 단점
  * **결과에 대한 해석이 쉽지 않음**
  * 최적의 모형을 도출하는 것이 상대적으로 어려움
  * 모형이 복잡하면 훈련 과정에 시간이 많이 소요됨
  * 데이터를 정규화 하지 않으면 지역해(local minimum)에 빠질 위험이 있음

### 신경망 활성화 함수

* 신경망 활성화 함수(activation function)
  * 결과값을 내보낼 때 사용하는 함수로, 가중치 값을 학습할 때 에러가 적게 나도록 도움
  * **풀고자 하는 문제 종류에 따라 활성화 함수의 선택이 달라짐**
  * 목표 정확도와 학습시간을 고려하여 선택하고 혼합 사용도 함
  * 문제 결과가 직선을 따르는 경향이 있으면 '선형함수'를 사용
* 활성화 함수의 종류
  * 계단함수 : 0 또는 1의 결과
  * 부호함수 : -1 또는 1의 결과
  * 선형함수
* sigmoid 함수
  * **연속형 0~1,** Logistic 함수라 불리기도 함
  * 선형적인 멀티 - 퍼셉트론에서 비선형 값을 얻기 위해 사용
* softmax 함수
  * 모든 logits의 합이 1이 되도록 output을 정규화
  * sigmoid 함수의 일반화된 형태로 결과가 다 범주인 경우 **각 범주에 속할 사후 확률을(posterior probability) 제공**하는 활성화 함수
  * 주로 3개 이상 분류시 사용함



### 신경망 은닉 층, 은닉 노드

* 다층신경망은 단층신경망에 비해 훈련이 어려움
* 은닉층 수와 은닉 노드 수의 결정은 '**분석가가 분석 경험에 의해 설정**' 함



* 은닉층 노드가 너무 **적으면**	
  * 네트워크가 복잡한 의사결정 경계를 만들 수 없음
  * underfitting 문제 발생
* 은닉층 노드가 너무 **많으면**
  * 복잡성을 잡아낼 수 있지만, 일반화가 어렵다
  * **레이어가 많아지면 기울기 소실 문제가 발생**할 수 있다.
  * **과적합(overfitting) 문제 발생**
* 역전파 알고리즘(backpropagation algorithm)
  * 출력층에서 제시한 값에 대해, 실제 원하는 값으로 학습하는 방법으로 사용
  * 동일 입력층에 대해 원하는 값이 출력되도록 개별의 weight를 조정하는 방법으로 사용됨
* 기울기 소실 문제(vanishing gradient problem)
  * 다층신경망에서 **은닉층이 많아** 인공신경망 기울기 값을 베이스로 하는 역전파 알고리즘으로 학습시키려고 할 때 발생하는 문제

### 기울기 소실

* 역전파 알고리즘은 출력층에서 입력층으로 오차 Gradient를 흘려보내면서, 각 뉴런의 입력 값에 대한 손실함수의 Gradient를 계산함
* 이렇게 계산된 Gradient를 사용하여 각 가중치 매개변수를 업데이트 해 줌
* 다층신경망에서는 역전파 알고리즘이 입력층으로 갈 수록 Gradient가 점차적으로 작아져 0에 수렴하여 weight가 업데이트 되지 않는 현상
* activation function으로 sigmoid 함수를 사용할 때 발생 -> 해결을 위해 ReLU등 다른 함수 사용



### 모형평가

* 홀드아웃(hold out)
  * **원천 데이터를 랜덤하게 두 분류로 분리하여 교차검정을 실시하는 방법**으로 하나는 모형 학습 및 구축을 위한 훈련용 자료로 다른 하나는 성과평가를 위한 검증용 자료로 사용하는 방법
  * 과적합(overfitting) 발생 여부를 확인하기 위해서 주어진 데이터의 일정 부분을 모델로 만드는 훈련데이터로 사용하고 나머지 데이터를 사용해 모델을 평가
  * 잘못된 가설을 가정하게 되는 2종의 오류의 발생을 방지
  * idx <- sample(2, nrow(iris), replace=TRUE, prob=c(0.7, 0.3))
    * iris데이터를 7:3비율로 나누어 training에서 70%, Testing에 30% 사용하도록 하는 것
* 교차검증(cross validation)
  * 데이터가 충분하지 않을 경우 Hold-out으로 나누면 많은 양의 분산 발생
  * 이에 대한 해결책으로 교차검증을 사용할 수 있음, 그러나 **클래스 불균형**(한쪽으로 몰려잇는 데이터) 데이터는 적합하지 않음
  * 주어진 데이터를 가지고 **반복적으로 성과를 측정하여 그 결과를 평균**한 것으로 분류 분석 모형의 **평가 방법**
* 붓스트랩(bootstrap)
  * 평가를 반복하는 측면에서 교차검증과 유사하지만 훈련용자료를 반복 재선정한다는 점에서 차이
  * 붓스트랩은 **관측치를 한 번 이상 훈련용 자료로 사용**하는 복원추출법에 기반함
  * 전체 데이터 양이 크지 않을 경우 모형 평가에 가장 적합
  * **훈련 데이터를 63.2% 사용하는 0.632붓스트랩**이 있음

* 데이터 분할 시 고려사항
  * class의 비율이 한쪽에 치우쳐 있는 클래스 불균형 상태라면 다음 기법 사용을 고려한다.
    * under sampling : 적은 class의 수에 맞추는 것
    * over sampling : 많은 class의 수에 맞추는 것



### 오분류표를 활용한 평가 지표

| confusion matrix |       | 예측값                      |                                               |                                             |
| ---------------- | ----- | --------------------------- | --------------------------------------------- | ------------------------------------------- |
|                  |       | TRUE                        | FALSE                                         |                                             |
| 실제값           | TRUE  | 40(TP)                      | 60(FN)<br />Type 2 Error                      | Sensitivity<br />TP / (TP+FN)               |
|                  | FALSE | 60(FP)<br />Type 1 Error    | 40(TN)                                        | Specificity<br />TN / (TN+FP)               |
|                  |       | Precision<br />TP / (TP+FP) | Negative Predictive Value<br />TN / (TN + FN) | Accuracy<br />(TP + TN) / (TP+TN + FP + FN) |



* 정밀도 Precision
  * 예측값이 True인 것에 대해 실제값이 True인 지표
  * 식 : TP / (TP+FP)
* 재형율, 민감도 Recall, Sensitivity
  * 실제값이 True인 것에 대해 예측값이 True인 
  * 식 : TP / (TP+FN)
* F1
  * F1은 데이터가 불균형 할 때 사용한다
  * 오분류표 중 정밀도와 재현율의 조화평균을 나타내면 정밀도와 재현율에 같은 가중치를 부여하여 평균한 지표
  * 2 * (Precision * Recall) / (Precision + Recall)
* Accuracy
  * (TP + TN) / (TP + FP + FN + TN)
  * 전체 예측에서 옳은 예측의 비율
* Error Rate
  * (FP + FN) / (TP + FP + FN + TN)
  * 전체 예측에서 틀린 예측의 비율
* 특이도 Specificity
  * TN / (FP + TN)
  * 실제로 N인 것들 중 예측이 N으로 된 경우의 비율
* FP Rate
  * FP / (FP + TN), 1 - Specificity
  * 실제가 N인데 예측이 P로 된 비율(Y가 아닌데 Y 로 예측된 비율, 1종오류)
* kappa
  * (Accuracy - P(e)) / (1 - P(e))     * P(e) : 우연히 일치할 확률
  * **두 평가자의 평가나 ㅇ러마나 일치하는지 평가하는 값으로 0~1 사이의 값을 가짐
* F2
  * 재현율에 정밀도의 2배 만큼 가중치를 부여하는 것
