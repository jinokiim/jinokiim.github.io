---
layout: post
title: Part 3
subtitle: 
gh-repo: jinokiim/jinokiim.github.io
gh-badge: [star, fork, follow]
thumbnail-img: /assets/img/sp/dspthumb.png
cover-img: /assets/img/sp/dspbg.jpeg
tags: [ADsP]
comments: true
---  

# 데이터 분석 기획



### 데이터 마이닝 개요

#### 데이터 마이닝

기업이 보유하고 있는 일일거래 데이터, 고객 데이터, 상붐데이터 혹은 각종 마케팅 활동에 있어서의 고객반응 데이터 등과 이외의 외부 데이터를 포함하는 **모든 사용가능한 원천 데이터를 기반으로 감춰진 지식, 기대하지 못했던 경향 또는 새로운 규칙 등을 발견하고 이를 실제 비즈니스 의사결정 등에 유용한 정보로 활용**하는 일련의 작업

#### 데이터 마이닝 5단계

* 목적 정의
  * 데이터 마이닝 도입 목적을 명확하게 함
* 데이터 준비
  * **데이터 정제(Cleaning)** 를 통해 데이터의 품질 확보까지 포함
  * 필요시 **데이터 양 충분하게 확보**
* 데이터 가공
  * **목적 변수를 정의하고, 필요한 데이터를 데이터 마이닝 소프트웨어에 적용할 수 있게 가공 및 준비하는 단계**
  * 충분한 CPU와 메모리, 디스크 공간 등 개발환경 구축이 선행
* 데이터 마이닝 기법 적용
  * 모델을 목적에 맞게 선택하고 소프트웨어를 사용하는 데 필요한 값 지정
* 검증
  * 결과에 대한 검증 시행



### 대표적 데이터 마이닝 기법

* 분류(Classification)
  * 새롭게 나타난 현상을 검토하여 **기존의 분류, 정의된 집합에 배정**하는 것
  * 의사결정나무, memory-based reasoning 등
* 추정(Estimation)
  * 주어진 입력 데이터를 사용하여 **알려지지 않은 결과의 값을 추정**하는 것
  * 연속된 변수의 값을 추정, 신경망 모형
* 연관분석 (Association Analysis)
  * **'같이 풀리는 물건'** 같이 아이템의 연관성을 파악하는 분석
  * **카탈로그 배열 및 교차판매, 공격적 판촉행사 등의 마케팅 계획**
* 예측(Prediction)
  * **미래**에 대한 것을 예측, 추정하는 것을 제외하면 분류나 추정과 동일한 의미
  * 장바구니 분석, 의사결정나무, 신경망 모형
* 군집(Clustering)
  * 미리 정의된 기준이나 예시에 의해서가 아닌 **레코드 자체**가 가진 다른 레코드와의 유사성에 의해 그룹화되고 이질성에 의해 세분화 됨
  * 데이터 마이닝이나 모델링의 준비단계로서 사용됨
* 기술(Description)
  * **데이터가 가진 특징 밑 의미를 단순하게 설명하는 것**
  * 데이터가 암시하는 바에 대해 설명 및 그에 대한 답을 찾아 낼 수 있어야 함



### 데이터 분석 순서

1. 분석용 데이터 준비

2. 탐색적 분석 데이터 전처리
   * 속성간 상관관계 파악
   * 데이터 특성 파악
   * 분포 파악
   * 데이터 확인
   * 데이터 형식 변경
   * 결측값, 이상값 처리
   * 특성조작
   * 데이터 자원 축소
3. 모델링
   * 회귀분석
   * 분류분석
   * 군집분석
   * 연관분석
4. 모델 평가 및 검증
   * 결정계수(R^2)
   * F통계량, t값
   * ROC Curve
   * 오분류표
   * 실루엣, DI
5. 모델 적응 운영방안 수립



### Machine Learning Algorithms

|             |                         Unsupervised                         |                          Supervised                          |
| :---------: | :----------------------------------------------------------: | :----------------------------------------------------------: |
| Continuous  | Clustering & Dimensionality Reduction<br />SVD<br />SVD<br />PCA<br />K-means |                Regression(Linear, Polynomial)                |
| Categorical |        Association Analysis<br />(Apriori, FP-Growth)        | Classification<br />(kNN ,DecisionTrees,<br />Logistic Regression, Naive-Bayes, SVM) |

### 분류분석의 종류

1. 로지스틱 회귀
2. 의사결정나무
3. 앙상블
4. 신경망 모형
5. kNN, 베이즈분류 모형, SVM(서포트벡터기계), 유전자 알고리즘



### 로지스틱 회귀분석

* 독립변수는 연속형, **종속변수가 범주형인 경우 적용되는 회귀분석** 모형
* 종속변수가 성공/실패, 사망/생존 과 같이 **이항변수(0,1)**로 되어있을때 종속변수와 독립변수 간의 관계식을 잉요하여 두 집단 또는 그 이상의 집단을 **분류하고자 할 때 사용**되는 분석기법
  * 선형회귀
    * x값의 변화에 따른 y값의 변화를 알아냄
    * x가 1증가할 때, y는 회귀 계수 만큼 증가함
  * 로지스틱회귀
    * x값에 따른 y값의 변화량의 문제가 아님
    * 회귀계수를 해석할 때 문제가 생김

|                |     일반 선형 회귀분석      | 로지스틱 회귀분석               |
| :------------: | :-------------------------: | ------------------------------- |
|    종속변수    |         연속형 변수         | 이산형(범주형) 변수             |
| 모형 탐색 방법 | 최소자승법(LSM, 최소제곱법) | 최대우도법(MLE), 가중최소자승법 |
|   모형 검정    |       F-test, T-test        | X^2 test                        |



종속변수를 전체 실수범위로 확정하여 분석하고, sigmoid함수를 사용해 연속형 0~1로 변경

* 확률 probability
  * 0~1사이의 값
* odd(승산)
  * 성공률 / 실패율, Pi / (1-Pi), Pi = 성공률
    * 성공이 일어날 가능성이 높은 경우는 1.0보다 큰 값
    * 실패가 발생할 가능성이 높은 경우는 1.0보다 작은 값
  * **로지스틱의 회귀계수, 확률에 대해 0~∞ 로 변환한 값**
* log odds(=logit)
  * log(odds), odds 값에 log를 취함
  * 선형화의 하나, 값의 검위를 **전체 실수 범위(-∞ ~ +∞)로 확장**
  * log(odds(p)) = wx + b 형태로 선형분석이 가능해짐
* sigmoid
  * log odds 값을 연속형 0~1사이의 값으로 바꾸는 함수
  * Logistic 함수라 불리기도 하며, 비선형적 값을 얻기 위해 사용

#### 회귀 식에 대한 해석 방법이 선형회귀와 다름

* 승산비(odds ratid)
  * 관심있는 사건이 발생할 상대 비율, x=1일떄, y=1이되는 상대적 비율



### 의사 결정 나무(Decision Tree)모형


* 의사결정 규칙을 나무구조로 나타내 전체 자료를 몇 개의 소집단으로 **분류**하거나 **예측을 수행**하는 분석 방법
* 분석과정이 직관적이고 이해하기 쉬움

#### 용어정리  
  
|                 독립변수                  |                종속변수                 |  
| :---------------------------------------: | :-------------------------------------: |  
| * 설명변수<br />* 예측변수<br />* Feature | * 목표변수<br />* 반응변수<br />* Label |



* 특징
  * 목적은 새로운 **데이터를 분류**하거나 **값을 예측**하는 것이다.
  * 분리 변수 P차원 공간에 대한 현재 분할은 **이전 분할에 영향을 받는다**
  * 부모마디보다 자식마디의 **순수도가 증가하도록 분류나무를 형성**해 나간다(**불순도 감소**)
* 종류
  * 목표변수(=종속변수)가 이산형인 경우 분류나무(classification tree)
  * 목표변수가 연속형인 경우 회귀나무(regression tree)
* 장점
  * 구조가 단순하여 해석이 용이함
  * **비모수적 모형으로** 선형성, 정규성, 등분산성 등의 수학적 가정이 불필요함
  * **범주형(이산형)과 수치형(연속형) 변수를 모두 사용**할 수 있음
* 단점
  * 분류 기준값의 경계선 부근의 자료 값에 대해서는 오차가 큼(비연속성)
  * 로지스틱회귀와 같이 각 예측변수의 효과를 파악하기 어려움
  * 새로운 자료에 대한 예측이 불안정할 수 있음



### 의사결정나무의 결정 규칙

* 분리기준 split criterion
  * 새로운 가지를 만드는 기준?
    * 순수도가 높아지는 방향으로 분리
    * 불확실성이 낮아지는 방향
* 정지규칙 stopping rule
  * 더 이상 분리가 일어나지 않고 현재 마디가 최종마디가 되도록 하는 규칙
  * '불순도 감소량'이 아주 작을 때 정지함
* 가지치기 규칙 pruning rule
  * 어느 가지를 쳐내야 예측력이 좋은 나무가 될까?
  * **최종 노드가 너무 많으면 overfitting 가능성이 커짐, 이를 해결하기 위해 사용**
  * 가지치기 규칙은 별도 규칙을 제공하거나 경험에 의해 실행할 수 있음
  * 가지치기의 비용함수(cost function)을 **최소로 하는 분기를 찾아내도록** 학습
  * Information Gain이란 어떤 속성을 선택함으로 인해 데이터를 더 잘 구분하게 되는것을 의미함(불확실성 감소)

### 불순도 측정 지표

#### 목표변수가 범주 형일 때 사용하는 지표(분류에서 사용)

* 지니지수
  * 불순도 측정 지표, **값이 작을수록 순수도가 높음(분류가 잘 됨)**
  * 가장 작은 값을 갖는 예측 변수와 이때의 최적 분리에 의해 자식 마디 형성
  * Gini(T) = 1 - ((각 범주별수 / 전체수)의 총합)^2
* 엔트로피 지수 Entropy measure
  * 불순도 측정 지표, **가장 작은 값을 갖는 방법 선택**
  * Entropy(T) = -|_sum_{(i=1);(k);(Pilog2Pi)}
* 카이제곱 통계량의 유의 확률(p-value)



### 의사결정나무를 위한 알고리즘

의사결정나무를 위한 알고리즘은 CHAID, CART, ID2, C5.0, C4.5가 있으며 **하향식 접근 방법**을 이용한다.

|                         알고리즘                         | 이산형 목표변수(분류나무) | 연속형 목표변수(회귀나무) |
| :------------------------------------------------------: | :-----------------------: | :-----------------------: |
|      CART<br />(classification and regression tree)      |         지니지수          |        분산감소량         |
|                           C5.0                           |       엔트로피지수        |                           |
| CHAID<br />(chi-squared automatic interaction detection) | 카이제곱 통계량의 p-value | ANOVA F-통계량 - p-value  |



### 앙상블(Ensemble) 모형

* **여러 개의 분류 모형에 의한 결과를 종합하여 분류의 정확도를 높이는 방법**
* 적절한 표추출법으로 데이터에서 여러 개의 훈련용 데이터 집합을 만들어 각 데이터 집합에 하나의 분류기를 만들어 결합하는 방법
* 약하게 학습된 여러 모델들을 결합하여 사용
* **성능을 분산시키기 때문에 과적합(overfitting) 감소 효과가 있음



#### 앙상블 모형의 종류

* Voting
  * **서로 다른 여러 개 알고리즘 분류기 사용**
  * 각 모델의 결과를 취합하여 많은 결과 또는 높은 확률로 나온 것을 최종 결과로 채택하는 것
  * Hard voting
    * 각 모델의 예측 결과중 많은 것을 선택
    * 1 예측 3표, 2예측 1표 -> 1예측 선택
  * Soft voting
    * 각 모델의 클래스 확률을 구하고 평균 높은 확률을 선택
    * 1예측 : (0.9 + 0.8 + 0.3 + 0.4) / 4 = 0.6 -> 1예측 선택
    * 2예측 : (0.1 + 0.2 + 0.7 + 0.6) / 4 = 0.4
* Bagging(Bootstrap AGGregatING)
  * **서로 다른 훈련 데이터 샘플로 훈련, 서로 같은 알고리즘 분류기 결합**
  * 원 데이터에서 중복을 허용하는 크기가 같은 표본을 여러번 단순 임의 복원 추출하여 각 표본에 대해 분류기 (classifiers)를 생성하는 기법
  * **여러 모델이 병렬로 학습, 그 결과를 집계하는 방식**
  * 같은 데이터가 여러번 추출될 수도 있고, 어떤 데이터는 추출되지 않을 수 있음
  * 대표적 알고리즘 : MetaCost Algorith,
* Boostinig
  * 여러 모델이 **순차적으로 학습**
  * 이전 모델의 결과에 따라 다음 모델 표본 추출에서 분류가 잘못된 데이터에 가중치(weight)를 부여하여 표본을 추출함
  * 맞추기 어려운 문제를 맞추는데 초점이 맞춰져 있고, 이상치(Outlier)에 약함
  * 대표적 알고리즘 : AdaBoost, GradientBoost(XGBoost, Light GBM) 등
* Random Forest
  * 배깅(Bagging)에 랜덤 과정을 추가한 방법
  * 노드 내 데이터를 자식 노드로 나누는 기준을 정할 때 모든 예측변수에서 최저그이 분할을 선택하는 대신, **설명변수의 일부분만을 고려**함으로 성능을 높이는 방법 사용
  * **여러 개 의사결정 나무를 사용해**, 하나의 나무를 사용할 때보다 **과적합 문제를 피할 수 있음**

 

### k-NN(k-Nearest Neighbors)

* 새로운 데이터에 대해 주어진 **이웃의 개수(k)** 만큼 가까운 멤버들과 비교하여 결과를 판단하는 방법
* k 값에 따라 소속되는 그룹이 달라질 수 있음(k값은 hyper parameter)
* 거리를 측정해 이웃들을 뽑기 때문에 **스케일링이 중요함**
* 반응 변수가 범주형이면 불뉴, 연속형이면 회귀의 목적으로 사용됨
* 모형을 미리 만들지 않고, 새로운 데이터가 들어오면 그때부터 계산을 시작하는 lazy learning(게으른 학습)이 사용되는 지도학습 알고리즘



### SVM(Support Vector Machine)

* 서로 다른 분류에 속한 데이터 간의 간격(margin)이 최대가 되는 선을 찾아 이를 기준으로 데이터를 분류하는 모델



### 인공 신경망(ANN) 모형

* 인공신경망을 이용하면 분류 및 군집을 할 수 있음
* 인공신경망은 입력층, 은닉층, 출력층 3개의 층으로 구성되어 있음
* 각 층에 뉴런(노드)이 여러 개 포함되어 있음
* 학습 : 입력에 대한 올바른 출력이 나오도록 가중치(weights)를 조절하는 것
* 가중치 초기화는 -1.0 ~ 1.0 사이의 임의 값으로 설정하며, 가중치를 지나치게 큰 값으로 초기화하면 활성화 함수를 편향 시키게 되며, **호라성화 함수가 과적합 되는 상태를 포화상태**라고 함



### 경사하강법(Gradient descent)

* **함수 기울기를 낮은 쪽으로 계속 이동**시켜 극값에 이를 때까지 반복시키는 것
* 제시된 함수의 기울기의 **최소값을 찾아내는** 머신러닝 알고리즘
* 비용함수(cost function)을 최소화 하기 위해 parameter를 반복적으로 조정하는 과정

#### 경사하강법 과정

1. 임이의 parameter값으로 시작
2. Cost Function 계산, cost function - 모델을 구성하는 가중치 w의 함수, 시작점에서 곡선의 기울기 계산
3. parameter 값 갱신 : W = W - learning rate * 기울기미분값
4. n 번의 iteration, 최소값을 향해 수렴함, learning rate가 적절해야함



### 인공 신경망 모형의 장/단점

* 장점
  * 변수의 수가 많거나 입,출력변수 간에 **복잡한 비선형 관계에 유용**
  * **이상치 잡음에 대해서도 민감하게 반응하지 않음**
  * 입력변수와 결과변수가 연속형이나 이산형인 경우 모두 처리 가능
* 단점
  * **결과에 대한 해석이 쉽지 않음**
  * 최적의 모형을 도출하는 것이 상대적으로 어려움
  * 모형이 복잡하면 훈련 과정에 시간이 많이 소요됨
  * 데이터를 정규화 하지 않으면 지역해(local minimum)에 빠질 위험이 있음

### 신경망 활성화 함수

* 신경망 활성화 함수(activation function)
  * 결과값을 내보낼 때 사용하는 함수로, 가중치 값을 학습할 때 에러가 적게 나도록 도움
  * **풀고자 하는 문제 종류에 따라 활성화 함수의 선택이 달라짐**
  * 목표 정확도와 학습시간을 고려하여 선택하고 혼합 사용도 함
  * 문제 결과가 직선을 따르는 경향이 있으면 '선형함수'를 사용
* 활성화 함수의 종류
  * 계단함수 : 0 또는 1의 결과
  * 부호함수 : -1 또는 1의 결과
  * 선형함수
* sigmoid 함수
  * **연속형 0~1,** Logistic 함수라 불리기도 함
  * 선형적인 멀티 - 퍼셉트론에서 비선형 값을 얻기 위해 사용
* softmax 함수
  * 모든 logits의 합이 1이 되도록 output을 정규화
  * sigmoid 함수의 일반화된 형태로 결과가 다 범주인 경우 **각 범주에 속할 사후 확률을(posterior probability) 제공**하는 활성화 함수
  * 주로 3개 이상 분류시 사용함



### 신경망 은닉 층, 은닉 노드

* 다층신경망은 단층신경망에 비해 훈련이 어려움
* 은닉층 수와 은닉 노드 수의 결정은 '**분석가가 분석 경험에 의해 설정**' 함



* 은닉층 노드가 너무 **적으면**	
  * 네트워크가 복잡한 의사결정 경계를 만들 수 없음
  * underfitting 문제 발생
* 은닉층 노드가 너무 **많으면**
  * 복잡성을 잡아낼 수 있지만, 일반화가 어렵다
  * **레이어가 많아지면 기울기 소실 문제가 발생**할 수 있다.
  * **과적합(overfitting) 문제 발생**
* 역전파 알고리즘(backpropagation algorithm)
  * 출력층에서 제시한 값에 대해, 실제 원하는 값으로 학습하는 방법으로 사용
  * 동일 입력층에 대해 원하는 값이 출력되도록 개별의 weight를 조정하는 방법으로 사용됨
* 기울기 소실 문제(vanishing gradient problem)
  * 다층신경망에서 **은닉층이 많아** 인공신경망 기울기 값을 베이스로 하는 역전파 알고리즘으로 학습시키려고 할 때 발생하는 문제

### 기울기 소실

* 역전파 알고리즘은 출력층에서 입력층으로 오차 Gradient를 흘려보내면서, 각 뉴런의 입력 값에 대한 손실함수의 Gradient를 계산함
* 이렇게 계산된 Gradient를 사용하여 각 가중치 매개변수를 업데이트 해 줌
* 다층신경망에서는 역전파 알고리즘이 입력층으로 갈 수록 Gradient가 점차적으로 작아져 0에 수렴하여 weight가 업데이트 되지 않는 현상
* activation function으로 sigmoid 함수를 사용할 때 발생 -> 해결을 위해 ReLU등 다른 함수 사용



### 모형평가

* 홀드아웃(hold out)
  * **원천 데이터를 랜덤하게 두 분류로 분리하여 교차검정을 실시하는 방법**으로 하나는 모형 학습 및 구축을 위한 훈련용 자료로 다른 하나는 성과평가를 위한 검증용 자료로 사용하는 방법
  * 과적합(overfitting) 발생 여부를 확인하기 위해서 주어진 데이터의 일정 부분을 모델로 만드는 훈련데이터로 사용하고 나머지 데이터를 사용해 모델을 평가
  * 잘못된 가설을 가정하게 되는 2종의 오류의 발생을 방지
  * idx <- sample(2, nrow(iris), replace=TRUE, prob=c(0.7, 0.3))
    * iris데이터를 7:3비율로 나누어 training에서 70%, Testing에 30% 사용하도록 하는 것
* 교차검증(cross validation)
  * 데이터가 충분하지 않을 경우 Hold-out으로 나누면 많은 양의 분산 발생
  * 이에 대한 해결책으로 교차검증을 사용할 수 있음, 그러나 **클래스 불균형**(한쪽으로 몰려잇는 데이터) 데이터는 적합하지 않음
  * 주어진 데이터를 가지고 **반복적으로 성과를 측정하여 그 결과를 평균**한 것으로 분류 분석 모형의 **평가 방법**
* 붓스트랩(bootstrap)
  * 평가를 반복하는 측면에서 교차검증과 유사하지만 훈련용자료를 반복 재선정한다는 점에서 차이
  * 붓스트랩은 **관측치를 한 번 이상 훈련용 자료로 사용**하는 복원추출법에 기반함
  * 전체 데이터 양이 크지 않을 경우 모형 평가에 가장 적합
  * **훈련 데이터를 63.2% 사용하는 0.632붓스트랩**이 있음

* 데이터 분할 시 고려사항
  * class의 비율이 한쪽에 치우쳐 있는 클래스 불균형 상태라면 다음 기법 사용을 고려한다.
    * under sampling : 적은 class의 수에 맞추는 것
    * over sampling : 많은 class의 수에 맞추는 것



### 오분류표를 활용한 평가 지표

| confusion matrix |       | 예측값                      |                                               |                                             |
| ---------------- | ----- | --------------------------- | --------------------------------------------- | ------------------------------------------- |
|                  |       | TRUE                        | FALSE                                         |                                             |
| 실제값           | TRUE  | 40(TP)                      | 60(FN)<br />Type 2 Error                      | Sensitivity<br />TP / (TP+FN)               |
|                  | FALSE | 60(FP)<br />Type 1 Error    | 40(TN)                                        | Specificity<br />TN / (TN+FP)               |
|                  |       | Precision<br />TP / (TP+FP) | Negative Predictive Value<br />TN / (TN + FN) | Accuracy<br />(TP + TN) / (TP+TN + FP + FN) |



* 정밀도 Precision
  * 예측값이 True인 것에 대해 실제값이 True인 지표
  * 식 : TP / (TP+FP)
* 재형율, 민감도 Recall, Sensitivity
  * 실제값이 True인 것에 대해 예측값이 True인 
  * 식 : TP / (TP+FN)
* F1
  * F1은 데이터가 불균형 할 때 사용한다
  * 오분류표 중 정밀도와 재현율의 조화평균을 나타내면 정밀도와 재현율에 같은 가중치를 부여하여 평균한 지표
  * 2 * (Precision * Recall) / (Precision + Recall)
* Accuracy
  * (TP + TN) / (TP + FP + FN + TN)
  * 전체 예측에서 옳은 예측의 비율
* Error Rate
  * (FP + FN) / (TP + FP + FN + TN)
  * 전체 예측에서 틀린 예측의 비율
* 특이도 Specificity
  * TN / (FP + TN)
  * 실제로 N인 것들 중 예측이 N으로 된 경우의 비율
* FP Rate
  * FP / (FP + TN), 1 - Specificity
  * 실제가 N인데 예측이 P로 된 비율(Y가 아닌데 Y 로 예측된 비율, 1종오류)
* kappa
  * (Accuracy - P(e)) / (1 - P(e))     * P(e) : 우연히 일치할 확률
  * **두 평가자의 평가나 ㅇ러마나 일치하는지 평가하는 값으로 0~1 사이의 값을 가짐
* F2
  * 재현율에 정밀도의 2배 만큼 가중치를 부여하는 것

### 분류모형 성능 평가

#### ROC(Receiver Operation Characteristic) Curve

* x축은 FP Rate(1-Specificity), y축은 민감도(Sensitivity)를 나타내 이 두 평가 값의 관계로 모형을 평가함
* ROC 그래프의 밑부분의 면겆 (AUC, Area Under the Curve)이 넓을수록 좋은 모형으로 평가함



### 군집분석(Clustering Analysis)의 종류

* 여러 변수 값들로부터 n개의 개체를 유사한 성격을 가지는 몇개의 군집으로 집단화하고 형성된 군집들의 특성을 파악해 군집들 사이의 관계를 분석하는 다량분석 기법



* 계층적 군집 Hierarchical
  * 응집형(Agglomerative)
    * Bottomm-up
    * **단일(최단) 연결법,** 완전(최장) 연결법, 평균 연결법, 중심 연결법, **Ward연결법**
  * 분리형(Divisive)
    * Top-Down
    * 다이아나 방법(DIANA Method)
* 분할적 군집 Partitional
  * 프로토타입 -기반
    * Prototype-based
    * k-중심 군집 : **k-평균 군집, k-중앙값 군집, k-메도이드 군집**
    * 퍼지(Fuzzy) 군집
  * 분포기반
    * Distribution-based
    * 혼합 분포 군집
  * 밀도기반
    * Density-based
    * 중심밀도 군집, 격자기반군집



### 계층적 군집(Hierarchical Clustering)

* 계층적 군집 분석의 특징
  * 가장 유사한 개체를 묶어 나가는 과정을 반복하여 원하는 개수의 군집을 형성하는 방법
  * 유사도 판단은 **두 개체 간의 거리에 기반하므로 거리측정에 대한 정의가 필요함**
    * 유클리드, 맨해튼, 민코프스키, 마할라노비스 등
  * **이상치에 민감함**
  * **사전에 군집수 k를 설정할 필요가 없는 탐색적 모형**
  * 군집을 형성하는 데 매 단계에서 지역적 최적화를 수행해 나가는 방법을 사용하므로 그 결과가 전역적인 최적해라고 볼 수 없음
  * 병학적 방법에서 **한 번 군집이 형성되면 군집에 속한 개체는 다른 군집으로 이동할 수 없음**
  * hclust()함수, cluster 패키지의 agnes(), mclust() 함수 사용 
* 계층적 군집 - 응집형(병합 군집)군집 방법
  * 최단연결법
    * **단일연결법** 이라고도 하며, 두 군집 사이의 거리를 군집에서 하나씩 관측값을 뽑았을 때 나타날 수 있는 **거리의 최솟값을 측정**, 고립된 군집을 찾는데 중점을 둔 방식
  * 최장연결법
    * **완전연결법** 이라고도 하며, 두 군집 사이의 거리를 군집에서 하나씩 관측 값을 뽑았을 때 나타날 수 있는 **거리의 최대값을 측정**
  * 중심연결법
    * 두 군집의 **중심 간의 거리를 측정함**
    * 두 군집이 결함될 때 새로운 군집의 평균은 가중평균을 통해 구해짐
  * 와드연결법
    * 계층적 군집내의 **오차제곱합에** 기초하여 군집을 수행하는 군집방법
    * 크기가 비슷한 군집끼리 병합하는 경향이 있음
  * 평균연결법
    * 모든 항목에 대한 거리 평균을 구하면서 군집화, **계산양이 많아질 수 있음**



### 계층적 군집의 거리

**수학적 거리 개념 : 유클리드, 맨해튼, 민코프스키**

통계적 거리개념 : 표준화, 마할라노비스



* 유클리드
  * 두 점 사이의 가장 직관적이고 일반적인 거리의 개념, 방향성이 고려되지 않음
* 맨해튼
  * 두 점의 각 성분별 차의 절대값 합
* 민코프스키
  * 거리차수와 함께 사용되며, 일반적으로 사용되는 거리 차수는 1, 2, ∞
  * q=2이면 Euclidean, q=1이면 Manhattan Distance
* 표준화거리
  * 각 변수를 해당 변수의 표준편차로 척도 변환한 후에 유클리드 거리를 계산한 것으로 통계적 거리(Statistical distance)라고도 함
  * 표준화를 하면 척도의 차이, 분산의 차이로 인한 왜곡을 피할 수 있음
* 마할라노비스
  * **변수의 표준화와 함께 변수 간의 상관성을 동시에 고려한 통계적 거리**
* dist함수
  * 거리측정에 사용하는 함수로 사용가능한 거리 개념으로 **유클리드, 맨해튼, 민코프스키, maximum, canberra, binary**등이 있음
* 코사인(cosine)거리
  * 두 벡터 사이의 사잇각을 계산해서 **유사한 정도를** 구하는 것
  * 값이 1인 경우 유사도가 크며, -1인 경우 유사도가 매우 작음을 의미함



### 비계층적 군집

#### 비계층적 군집 - 분할적 군집 방법 : k-중심 군집

* k-means
  * **k-mean 방법은 사전에 군집의 수 k를 정해 주어야 함(k: hyperparameter)**
  * 군집수 k가 원데이터 구조에 적합하지 않으면 좋은 결과를 얻을 수 없음
  * 알고리즘이 단순하며 빠르게 수행되어 계층적 군집보다 많은 양의 자료를 처리
  * k-mean 군집은 잡음이나 이상값에 영향을 받기 쉬움
  * k-mean 분석 전에 이상값을 제거하는 것도 좋은 방법
  * 평균 대신 중앙값을 사용하는 k-medoids 군집을 사용할 수 있음
* k-means 절차
  1. 초기 군집의 중심으로 k개의 객체를 임의로 선택한다.
  2. 각 자료의 가장 가까운 군집의 중심에 할당한다.
  3. 각 군집 내의 자료들의 평균을 계산하여 군집의 중심을 갱신한다.
  4. 군집 중심의 변화가 거의 없을 때까지 2, 3을 반복한다.

* DBSCAN
  * **밀도기반** 클러스터링으로 점이 세밀하게 몰려 있어 밀도가 높은 부분을 클러스터링이라 한다.
  * 어느 점을 기준으로 **반경 내에 점이 n개 이상 있으면 하나의 군집으로 인식하는 방식**
  * Gaussian 분포가 아닌 **임의적 모양의 군집**분석에 적합함
  * k 값을 정할 필요 없음, outlier에 의한 성능 하락을 완화할 수 있음
* 혼합분포군집
  * 데이터가 봉우리가 2개인 분포, 도넛형태의 분포 등 복잡한 형태를 가진 분포의 경우 여러 분포를 확률적으로 선형 결합한 혼합분포로 설명될 수 잇음
  * 데이터가 k개의 모수적 모형의 가중합으로 표현되는 모집단 모형에서 나왔다는 가정하에, 추정된 k개의 모형 중 어느 모형으로부터 나왔을 확률이 높은지에 따라 군집 분류를 수행
  * 모수와 가중치 추정에 EM알고리즘이 사용됨(Expectation Maximization)
* EM 알고리즘
  1. 모수(평균, 분산, 혼합계수)에 대해 임의의 초기값을 정함
     * 잠재변수(latent variable) : 어느 집단에 속하는지에 대한 정보를 갖는 변수
  2. E step : k개의 모형 군집에 대해 모수를 사용해 각 군집에 속할 사후확률을 구함
  3. M step : 사후확률을 이용해 최대 우도 추정으로 모수를 다시 추정하고, 이를 반복함



### 군집화 평가 지수

* 실루엣 계수(silhouette coefficient)
  * 실루엣 지표가 1에 가까울수록 군집화가 잘 되었다고 판단
  * 실루엣 지표가 1 : 한 군집의 모든 개체가 한치도 떨어져 있지 않고 붙어있는 경우
  * 실루엣 지표가 0.5 보다 크면 결과가 타당한 것으로 평가
* Dunn Index(DI)
  * 군집과 군집 사이 거리 중 최솟값 / 군집 내 데이터들 거리 중 최댓값
  * 분자가 클 수록 군집간 거리가 멀고, 분모가 클 수록 군집 내 데이터가 모여 있음
  * Dunn Index 가 클수록 군집화가 잘 되었다고 평가



### SOM(Self-Organizing Maps)

* Som

  * 자기조직화지도
  * 인공신경망의 한 종류로, **차원축소와 군집화를 동시에 수행**하는 기법
  * **비지도학습(Unsupervised Learning)**의 한가지 방법
  * 고차원으로 표현된 데이터를 **저차원으로 변환**해서 보는데 유용함
  * 입력층과 2차원의 격차 형태의 경쟁층(=출력층)으로 이루어져 있음(2개의 층으로 구성)

* SOM Process

  1단계 : SOM의 노드에 대한 연결강도(weight) 초기화

  2단계 : 입력 벡터와 경쟁 층 노드 간의 거리 계산 및 입력벡터와 가까운 노드 선택 -> 경쟁

  3단계 : 경쟁에서 선택된 노드와 이웃 노드의 가중치(연결강도) 갱신 -> 협력 및 적응

  4단계 : 2단계로 가서 반봅

  **승자만이 출력을 내고, 승자와 그의 이웃만이 연결강도를 수정하는 승자 독점 구조로 인해 경쟁층에는 승자 뉴런만 나타남**

* SOM vs 신경망 모형
  * 신경망 모형은 연속적인 layer로 구성된 반면, **SOM은 2차원의 그리드(격자)로 구성**
  * 신경망 모형은 에러 수정을 학습하는 반면 **SOM은 경쟁학습 실시** 
  * 신경망은 역전파 알고리즘이지만, **SOM은 전방패스를 사용**해 속도가 매우 빠름

### 연관분석(Association Analysis)

* 연관분석
  * 연관규칙(Association rule) : 항목들 간의 '조건-결과' 식으로 펴현되는 유용한 패턴
  * 이러한 패턴 규칙을 발견해내는 것을 연관분석이라 함
  * 장바구니 분석이라고 함
* Apriori 알고리즘
  * 연관규칙의 대표적 알고리즘으로 현재도 많이 사용됨
  * 데이터들에 대한 **발생 빈도를 기반**으로 각 데이터 간의 연관관계를 밝히는 방법
  * 데이터셋이 큰 경우 모든 후보 itemset에 대해 하나하나 검사하는 것이 비효율적임
* FP Growth
  * Apriori 단점을 보완하기 위해 FP-tree 와 node, link 라는 특별한 자료구조를 사용
* 장점
  * **조건반응(if-then)**으로 표현되는 연관 분석의 결과라ㅡㄹ 이해하기 쉬움
  * 강력한 비목적성 분석 기법이며, 분석 계산이 간편함
* 단점
  * 분석 품목 수가 증가하면 분석 계산이 기하급수적으로 증가함
  * 너무 세분화된 품목을 가지고 연관규칙을 찾으려면 의미없는 분석 결과가 도출됨
  * 상대적 거래량이 적으면 규칙 발견 시 제외되기 쉬움

### 연관규칙 측정지표

* 규칙표기 : A -> B
  * if A then B -> A가 팔리면 B가 같이 팔린다.
* 지지도 Support
  * **전체 거래학목 중 상품 A와 B를 동시에 포함하여 거래하는 비율**
  * 전체 거래 중 차지하는 비율을 통해 해당 연관 규칙이 얼마나 의미가 있는 것인지를 확인함
  * **지지도 = P(A|B) : A와 B가 동시에 포함된 거래수 / 전체 거래수**
* 신뢰도 Confidence
  * **상품 A를 포함하는 거래 중 A와 B가 동시에 거래되는 비율**
  * 상품 A를 구매했을 때 상품 B를 구매할 확률이 어느정도 되는지를 확인
  * **신뢰도 = P(B|A) = P(A|B) / P(A) : A와 B가 동시에 포함된 거래수 / A가 포함된 거래수**
* 향상도 lift
  * A가 주어지지 않았을 때 B의 확률 대비 A가 주어졌을 때 B의 확률 증가 비율
  * 품목B를 구매한 고객 대비 품목 A를 구매한 후 품목 B를 구매하는 고객에 대한 확률
  * **향상도 = P(B|A) / P(B) = P(A|B) / P(A) * P(B)**
  * = 상품 A의 거래 중 상품 B가 포함된 거래의 비율 / 전체 상품 거래 중 상품 B가 거래된 비율
  * = A와 B가 동시에 일어난 확률 / A, B가 독립된 사건일 때 A, B가 동시에 일어날 확률
* 향상도 해석
  * 향상도 > 1
    * 향상도가 1보다 높아질수록 **연관성이 높다**고 할 수 있음
    * 이 규칙은 결과를 예측하는데 우수함
    * 서로 양의 관계로 품목 B를 구매할 확률보다 품목 A를 구매한 후 품목 B를 구매할 확률이 더 높다는 것을 의미함
  * 향상도 = 1
    * 품목A와 B사이에 아무런 **상호 관계가 없음(독립)**
  * 향상도 < 0
    * 향상도가 **음수**이면 두 품목이 **서로 음의 상관관계**임을 의미함

